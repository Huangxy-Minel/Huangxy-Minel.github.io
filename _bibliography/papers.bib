---
---
@inproceedings{ceio-sigcomm,
  abbr     = {SIGCOMM},
  year     = {2025},
  series   = {SIGCOMM 2025},
  title    = {CEIO: A Cache-Efficient Network I/O Architecture for NIC-CPU Data Paths},
  author   = {Liu*, Bowen and Huang*, Xinyang and Li, Qijing and Huang, Zhuobin and Sun, Yijun and Li, Wenxue and Zhang, Junxue and Yin, Ping and Chen, Kai},
  booktitle  = {ACM SIGCOMM 2025 (SIGCOMM '25)},
  address    = {},
  selected = {true},
  abstract = {Efficient Input/Output (I/O) data path between NICs and CPUs/DRAMs is critical for supporting datacenter applications with high-performance network transmission, especially as link speed scales to 100Gbps and beyond. Traditional I/O acceleration strategies, such as Data Direct I/O (DDIO) and Remote Direct Memory Access (RDMA), perform suboptimally due to the inefficient utilization of the Last-Level Cache (LLC). This paper presents CEIO, a novel cache-efficient network I/O architecture that employs proactive rate control and elastic buffering to achieve zero LLC misses in the I/O data path while ensuring the effectiveness of DDIO and RDMA under various network conditions. We have implemented CEIO on commodity SmartNICs and incorporated it into widely-used DPDK and RDMA libraries. Experiments with well-optimized RPC framework and distributed file system under realistic workloads demonstrate that CEIO achieves up to 2.9x higher throughput and 1.9x lower P99.9 latency over prior work.}
}

@inproceedings{dcp-sigcomm,
  abbr     = {SIGCOMM},
  year     = {2025},
  series   = {SIGCOMM 2025},
  title    = {Revisiting RDMA Reliability for Lossy Fabrics},
  author   = {Li, Wenxue and Liu, Xiangzhou and Zhang, Yunxuan and Wang, Zihao and Gu, Wei and Zeng, Gaoxiong and Ren, Shoushou and Huang, Xinyang and Ren, Zhenghang and Liu, Bowen and Zhang, Junxue and Chen, Kai},
  booktitle  = {ACM SIGCOMM 2025 (SIGCOMM '25)},
  address    = {},
  selected = {false}
}

@inproceedings{carc-apnet,
  abbr     = {APNet},
  year     = {2025},
  series   = {APNet 2025},
  title    = {Cache-Aware I/O Rate Control for RDMA},
  author   = {Li, Qijing and Huang, Xinyang and Liu, Bowen and Li, Pengbo and Zhang, Junxue and Chen, Kai},
  booktitle  = {9th Asia-Pacific Workshop on Networking (APNet '25)},
  address    = {},
  selected = {true},
  abstract = {Remote Direct Memory Access (RDMA) has become a cornerstone technology in modern datacenter networks due to its high throughput and extremely low latency. However, recent works have revealed that congestion arises in the "last mile" of the RDMA I/O path—--between DRAM and CPU registers--—due to inefficiencies in the memory hierarchy, where severe cache misses and memory bandwidth contention degrade performance. We identify the root cause of this I/O congestion as the speed mismatch between network ingress and CPU processing, which leads to data accumulation and, eventually, Last-Level Cache (LLC) overflow. To address this issue, we propose RhyR, a credit-based rate control mechanism that dynamically aligns network ingress speed with CPU processing speed. Our preliminary evaluation on eRPC over RDMA, a widely used RPC framework, demonstrates that RhyR effectively mitigates I/O congestion, reducing tail latency by up to 1.40x and improving throughput by up to 1.35x compared to prior work.}
}

@inproceedings{fuseline-osdi,
  abbr      = {OSDI},
  year      = {2025},
  series    = {OSDI 2025},
  title     = {Enabling Efficient GPU Communication over Multiple NICs with FuseLink},
  author    = {Ren, Zhenghang and Li, Yuxuan and Wang, Zilong and Huang, Xinyang and Li, Wenxue and Xu, Kaiqiang and Liao, Xudong and Sun, Yijun and Liu, Bowen and Tian, Han and Zhang, Junxue and Wang, Mingfei and Zhong, Zhizhen and Liu, Guyue and Zhang, Ying and Chen, Kai},
  booktitle = {Proceedings of the 19th USENIX Symposium on Operating Systems Design and Implementation},
  address    = {},
  code     = {https://github.com/axio-project/FuseLink},
  selected = {true},
  abstract = {Machine learning (ML) clusters stack multiple network interface cards (NICs) within each server to improve GPU communication bandwidth. However, existing systems fall short in fully utilizing NICs because of statically binding GPU traffic to NICs and PCIe bottleneck. This leads to suboptimal performance under imbalanced traffic, such as when GPUs process different LLM serving requests and training models with varying communication pattern. We propose FuseLink to enable efficient GPU communication over multiple NICs. FuseLink extends inter-server network by integrating high-speed intra-server connections, and recognizes GPUs to efficiently relay traffic to idle NICs. We implement FuseLink and integrate it into NCCL, so that ML applications can use FuseLink seamlessly without code modifications. Compared to NCCL with PXN, we verify that FuseLink can achieve 212GBps bandwidth between two inter-server GPUs and bring speedup on producing the first token in LLM model serving by 1.06-2.89, mixture-of-expert (MoE) training by up to 1.3x, and recommendation model training by up to 1.2x.}
}

@inproceedings{genibatch-eurosys,
  abbr      = {EuroSys},
  year      = {2024},
  series    = {EuroSys 2024},
  title     = {Accelerating privacy-preserving machine learning with GeniBatch},
  author    = {Huang, Xinyang and Zhang, Junxue and Cheng, Xiaodian and Zhang, Hong and Jin, Yilun and Hu, Shuihai and Tian, Han and Chen, Kai},
  booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
  pdf      = {genibatch-eurosys24.pdf},
  url      = {https://dl.acm.org/doi/abs/10.1145/3627703.3629563?casa_token=OGfraB9IkS4AAAAA%3A_V3gCLmG7-KBC0DiS0XbLa116pN79gIlJvHifeeRV8VFEjrpACbmydawUlY4YsFJUyRbYHVcFFmQyg},
  code     = {https://github.com/Huangxy-Minel/GeniBatch},
  selected = {true},
  abstract = {Cross-silo privacy-preserving machine learning (PPML) adopts Partial Homomorphic Encryption (PHE) for secure data combination and high-quality model training across multiple organizations (e.g., medical and financial). However, PHE introduces significant computation and communication overheads due to data inflation. Batch optimization is an encouraging direction to mitigate the problem by compressing multiple data into a single ciphertext. While promising, it is impractical for a large number of cross-silo PPML applications due to the limited vector operations support and severe data corruption.
  In this paper, we present GeniBatch, a batch compiler that translates a PPML program with PHE into an efficient program with batch optimization. GeniBatch adopts a set of conversion rules to allow PHE programs involving all vector operations required in cross-silo PPML and ensures end-to-end result consistency before/after compiling. By proposing bit-reserving algorithms, GeniBatch avoids bit-overflow for the correctness of compiled programs and maximizes the compression ratio. We have integrated GeniBatch into FATE, a representative cross-silo PPML framework, and provided SIMD APIs to harness hardware acceleration. Experiments across six popular applications show that GeniBatch achieves up to 22.6x speedup and reduces network traffic by 5.4x-23.8x for generic cross-silo PPML applications.}
}

@inproceedings{leo-infocom,
  abbr      = {INFOCOM},
  title     = {A Generic and Efficient Communication Framework for Message-level In-Network Computing},
  author    = {Wan, Xinchen and Li, Luyang and Tian, Han and Liao, Xudong and Huang, Xinyang and Zeng, Chaoliang and Wang, Zilong and Yang, Xinyu and Cheng, Ke and Ning, Qingsong and Liu, Guyue and Luo, Layong and Chen, Kai},
  booktitle = {Proceedings of the IEEE International Conference on Computer Communications},
  series    = {INFOCOM 2025},
  pdf       = {leo-infocom25.pdf},
  selected  = {false},
  year      = {2025}
}



